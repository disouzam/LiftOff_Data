<h1 align="center">üìä LiftOff Data</h1> 
<div align="center">
    <a href="https://www.python.org/" target="_blank"><img src="https://img.shields.io/badge/Python-14354C?style=for-the-badge&logo=python&logoColor=white" target="_blank"></a>
    <a href="https://www.postgresql.org/docs/" target="_blank"><img src="https://img.shields.io/badge/postgres-%23316192.svg?style=for-the-badge&logo=postgresql&logoColor=white" target="_blank"></a>
    <a href="https://fastapi.tiangolo.com/" target="_blank"><img src="https://img.shields.io/badge/FastAPI-005571?style=for-the-badge&logo=fastapi" target="_blank"></a>
    <a href="https://streamlit.io/" target="_blank"><img src="https://img.shields.io/badge/Streamlit-FF4B4B?style=for-the-badge&logo=streamlit&logoColor=white" target="_blank"></a>
    <a href="https://www.sqlalchemy.org/" target="_blank"><img src="https://img.shields.io/badge/SQLAlchemy-323232?style=for-the-badge&logo=sqlalchemy&logoColor=white" target="_blank"></a>
    <a href="https://pydantic-docs.helpmanual.io/" target="_blank"><img src="https://img.shields.io/badge/Pydantic-3776AB?style=for-the-badge&logo=pydantic&logoColor=white" target="_blank"></a>
    <a href="https://docs.docker.com/" target="_blank"><img src="https://img.shields.io/badge/docker-%230db7ed.svg?style=for-the-badge&logo=docker&logoColor=white" target="_blank"></a>
</div>


> Este projeto apresenta uma arquitetura de pipeline de dados de baixo custo, projetada para startups que precisam processar e analisar dados de vendas de forma eficiente.

---

### **Introdu√ß√£o**

Este projeto descreve uma arquitetura de pipeline de dados de baixo custo voltada para startups, com foco em integra√ß√£o de dados de vendas a partir de APIs e CRMs, utilizando tecnologias modernas e acess√≠veis. O objetivo √© criar uma solu√ß√£o escal√°vel para ingest√£o, transforma√ß√£o e visualiza√ß√£o de dados, garantindo que tanto engenheiros de dados quanto analistas possam colaborar eficientemente. A arquitetura proposta inclui a divis√£o do pipeline em m√∫ltiplas camadas (Bronze, Silver e Gold), integra√ß√£o com APIs, Kafka para streaming, Airbyte para ingest√£o de dados, Airflow para orquestra√ß√£o e DBT para transforma√ß√£o de dados. A plataforma colaborativa "Briefer" tamb√©m √© integrada, permitindo que analistas de dados acessem e utilizem os dados transformados de forma eficiente.

<p align="center">
<img src = "./img/arquitetura_1.2.png">
</p>

### **Sequence Diagram**

O diagrama abaixo ilustra a intera√ß√£o entre as principais camadas e componentes da arquitetura, desde a ingest√£o dos dados brutos at√© sua transforma√ß√£o e disponibiliza√ß√£o para an√°lise.
```mermaid
sequenceDiagram
    participant U as Usu√°rio
    participant SW as Sistema Web Streamlit
    participant V as Valida√ß√£o Pydantic
    participant DB as Banco de Dados SQLAlchemy
    
    U ->> SW: Inserir Dados
    SW ->> V: Enviar Dados para Valida√ß√£o
    alt Dados V√°lidos
        V ->> DB: Inserir Dados no Banco de Dados
        DB ->> SW: Confirma√ß√£o de Armazenamento
        SW ->> U: Dados Salvos com Sucesso
    else Dados Inv√°lidos
        V ->> SW: Retornar Erros de Valida√ß√£o
        SW ->> U: Exibir Mensagem de Erro
    end
```

O diagrama a seguir descreve o fluxo de dados desde a entrada do usu√°rio no frontend at√© a valida√ß√£o dos dados e o salvamento no banco de dados, se aprovado.

```mermaid
graph TD
    A[Usu√°rio Digita no Frontend] --> B{Valida√ß√£o do Contrato de Dados}
    
    B -- Dados V√°lidos --> C[Salva no Banco de Dados]
    B -- Dados Inv√°lidos --> D[Erro de Valida√ß√£o Exibido]

    A --> |Entrada de Dados| B
    B --> |Valida√ß√£o pelo Pydantic| C
    B --> |Falha na Valida√ß√£o| D
    C --> E[Dados Salvos com Sucesso]
    D --> F[Mostrar Mensagem de Erro no Frontend]
```

---

### **Tecnologias Utilizadas**

#### **Streamlit**

- **Descri√ß√£o:** Streamlit √© uma biblioteca Python de c√≥digo aberto que permite a cria√ß√£o de aplicativos web interativos de forma r√°pida e f√°cil. Utilizado principalmente para construir dashboards e interfaces de dados, o Streamlit √© ideal para prototipagem r√°pida e visualiza√ß√£o de dados sem a necessidade de conhecimentos avan√ßados em desenvolvimento web.
- **Uso no Projeto:** Utilizado para construir o frontend da aplica√ß√£o, permitindo que os usu√°rios insiram dados de vendas de forma interativa e visualizem os resultados diretamente na interface.

#### **Pydantic**

- **Descri√ß√£o:** Pydantic √© uma biblioteca de valida√ß√£o de dados que utiliza modelos baseados em classes Python para garantir que os dados inseridos estejam no formato correto. √â amplamente utilizada para valida√ß√£o e serializa√ß√£o de dados, garantindo integridade e consist√™ncia.
- **Uso no Projeto:** Pydantic √© utilizado para validar os dados inseridos pelos usu√°rios no frontend, garantindo que as informa√ß√µes estejam corretas antes de serem processadas e salvas no banco de dados.

#### **Psycopg2**

- **Descri√ß√£o:** Psycopg2 √© uma biblioteca que permite a intera√ß√£o com bancos de dados PostgreSQL diretamente atrav√©s de Python, facilitando a execu√ß√£o de comandos SQL e o gerenciamento das conex√µes.
- **Uso no Projeto:** Utilizado para conectar a aplica√ß√£o ao banco de dados PostgreSQL, executar comandos SQL, e salvar os dados validados.

#### **SQLAlchemy**

- **Descri√ß√£o:** SQLAlchemy √© uma poderosa biblioteca de SQL toolkit e ORM (Object-Relational Mapping) para Python. Ele permite a intera√ß√£o com bancos de dados relacionais de forma mais intuitiva, utilizando objetos Python em vez de comandos SQL diretamente.
- **Uso no Projeto:** SQLAlchemy foi utilizado para gerenciar a conex√£o com o banco de dados PostgreSQL e facilitar as opera√ß√µes de CRUD.

#### **MkDocs**

- **Descri√ß√£o:** MkDocs √© uma ferramenta est√°tica de documenta√ß√£o em Python que permite a cria√ß√£o de sites de documenta√ß√£o de forma simples e estruturada. √â especialmente √∫til para projetos que precisam de uma documenta√ß√£o clara e acess√≠vel para os desenvolvedores e usu√°rios.
- **Uso no Projeto:** MkDocs √© utilizado para gerar a documenta√ß√£o do sistema, detalhando como o projeto foi estruturado, as funcionalidades desenvolvidas, e como o sistema deve ser mantido e atualizado.
- **Configura√ß√£o e Execu√ß√£o:**
  1. Para visualizar a documenta√ß√£o localmente, execute: `mkdocs serve`
  2. Acesse a documenta√ß√£o em: `http://127.0.0.1:8000/`

<p align="center">
  <img src = "./img/mkdocs.png">
</p>

#### **MinIO**
- **Descri√ß√£o:** MinIO √© uma solu√ß√£o de armazenamento de objetos de c√≥digo aberto compat√≠vel com o protocolo Amazon S3, ideal para o armazenamento escal√°vel de grandes volumes de dados. Ele permite o uso local de servi√ßos de armazenamento distribu√≠do para dados n√£o estruturados, como arquivos, logs e objetos de dados.
- **Uso no Projeto:** No contexto deste projeto, MinIO √© utilizado para armazenar dados brutos de produtos, funcionarios e fornecedores, por exempo: imagens, documentos em pdf. Ele serve como a camada de armazenamento persistente dos dados nas fases de ingest√£o e transforma√ß√£o.

#### **Airbyte**

- **Descri√ß√£o:** Airbyte √© uma plataforma de integra√ß√£o de dados de c√≥digo aberto que permite conectar facilmente APIs, bancos de dados e outros sistemas para ingest√£o de dados em tempo real.
- **Uso no Projeto:** Respons√°vel pela ingest√£o de dados a partir de diferentes APIs e fontes de dados, garantindo que os dados sejam movidos para as camadas corretas do pipeline.

#### **Kafka**

- **Descri√ß√£o:** Apache Kafka √© uma plataforma de streaming distribu√≠da que permite a publica√ß√£o e assinatura de fluxos de dados em tempo real.
- **Uso no Projeto:** Utilizado para gerenciar o fluxo de dados de maneira escal√°vel e garantir que os dados sejam processados de forma cont√≠nua e eficiente.

#### **Airflow**

- **Descri√ß√£o:** Apache Airflow √© uma plataforma de orquestra√ß√£o de workflows que permite o agendamento e monitoramento de pipelines de dados.
- **Uso no Projeto:** Orquestra a execu√ß√£o de todos os componentes do pipeline, desde a ingest√£o at√© a transforma√ß√£o dos dados.

#### **DBT**

- **Descri√ß√£o:** DBT (Data Build Tool) √© uma ferramenta de transforma√ß√£o de dados que permite a constru√ß√£o de modelos SQL e a aplica√ß√£o de boas pr√°ticas de desenvolvimento de software ao ETL.
- **Uso no Projeto:** Utilizado para transformar os dados das camadas Bronze e Silver, preparando-os para a camada Gold, onde estar√£o prontos para consumo pelos analistas.

#### **Briefer**

- **Descri√ß√£o:** Briefer √© uma plataforma colaborativa de dados que permite que equipes de analistas acessem, compartilhem e analisem dados de maneira colaborativa.
- **Uso no Projeto:** Facilita o acesso e a explora√ß√£o dos dados transformados pelos analistas, proporcionando um ambiente colaborativo para an√°lise de dados.

<p align="center">
<img src = "./img/briefer.png">
</p>

---

### **Estrutura do Projeto**

#### **Divis√£o dos M√≥dulos**

O projeto est√° dividido em m√≥dulos para organizar melhor o desenvolvimento e facilitar a manuten√ß√£o futura. A seguir, est√£o os principais m√≥dulos do projeto:

1. **Frontend (`app.py`):**
   - Respons√°vel pela interface do usu√°rio onde os dados de vendas s√£o inseridos e exibidos.
   - Desenvolvido com Streamlit para proporcionar uma intera√ß√£o simples e amig√°vel.

2. **Contrato (`<model_name>_schema.py.py`):**
   - Define as regras de valida√ß√£o dos dados utilizando Pydantic.
   - Assegura que os dados inseridos no frontend est√£o no formato correto e cumprem as regras estabelecidas pelo sistema.

3. **Banco de Dados (`database.py`):**
   - Gerencia a conex√£o e as opera√ß√µes com o banco de dados PostgreSQL utilizando Psycopg2.
   - Facilita a intera√ß√£o com o banco sem a necessidade de escrever SQL diretamente.

### Diagrama de Fluxo das Camadas Bronze, Silver e Gold no DBT

O pipeline de dados √© dividido em tr√™s camadas principais para garantir a qualidade e integridade dos dados √† medida que eles progridem no sistema:

```mermaid
graph TD
    A[Raw Data Source] -->|Extrai dados brutos| B[Bronze Layer]
    B[Bronze Layer] -->|Limpeza de dados| C[Silver Layer]
    C[Silver Layer] -->|Agrega√ß√£o e c√°lculos| D[Gold Layer - Vendas por Produto]
    C[Silver Layer] -->|Agrega√ß√£o e c√°lculos| E[Gold Layer - Vendas por Vendedor]
    
    subgraph Bronze
        B1[bronze_vendas.sql]
    end
    
    subgraph Silver
        S1[silver_vendas.sql]
    end
    
    subgraph Gold
        G1[gold_vendas_7_dias.sql]
        G2[gold_vendas_por_vendedor.sql]
    end
```

### Explica√ß√£o do Diagrama

- **Bronze Layer:** Esta camada recebe os dados brutos diretamente das fontes, criando uma visualiza√ß√£o inicial sem transforma√ß√µes significativas.
  
- **Silver Layer:** Nesta etapa, os dados s√£o limpos, ajustando datas inv√°lidas e removendo outliers. √â a fase em que os dados come√ßam a ser preparados para an√°lise.

- **Gold Layer:** Dados finais prontos para an√°lise e visualiza√ß√£o, acess√≠veis por ferramentas como o Briefer.
  - **Gold Vendas por Produto:** Agrega e calcula os dados para apresentar o desempenho dos produtos nos √∫ltimos 7 dias.
  - **Gold Vendas por Vendedor:** Apresenta o desempenho dos vendedores, tamb√©m focando nos √∫ltimos 7 dias.

---

### **Passos para Configura√ß√£o e Execu√ß√£o**

## Instala√ß√£o via docker
Antes de rodar o Docker, crie um arquivo .env na raiz do projeto com os seguintes valores:

```
DB_HOST_PROD = postgres
DB_PORT_PROD = 5432
DB_NAME_PROD = mydatabase
DB_USER_PROD = user
DB_PASS_PROD = password
PGADMIN_EMAIL = email_pgadmin
PGADMIN_PASSWORD = password_pgadmin
```

Para iniciar a aplica√ß√£o, execute:

```bash
docker-compose up -d --build
```

#### **1. Criar o Reposit√≥rio**

- **Passo:** Inicie um novo reposit√≥rio no GitHub ou GitLab para versionar o projeto.
- **Comando:**
  ```bash
  git init
  ```

#### **2. Escolher a Vers√£o do Python para 3.12.3**

- Utilize `pyenv` para gerenciar e definir a vers√£o correta do Python:
  ```bash
  pyenv install 3.12.3
  pyenv local 3.12.3
  ```

#### **3. Criar um Ambiente Virtual**

- **Passo:** Crie um ambiente virtual para isolar as depend√™ncias do projeto.
- **Comando:**
  ```bash
  python3.12 -m venv .venv
  ```

#### **4. Entrar no Ambiente Virtual**

- **Comando:**
  - **Windows:**
    ```bash
    .venv\Scripts\activate
    ```
  - **Linux/Mac:**
    ```bash
    source .venv/bin/activate
    ```

#### **5. Instalar as Depend√™ncias**

- **Instalar os pacotes necess√°rios:**
  ```bash
  pip install -r requirements.txt
  ```

#### **6. Executar o Frontend**

- **Comando para rodar o frontend com Streamlit:**
  ```bash
  streamlit run app.py
  ```

#### **7. Configurar o PostgreSQL**

- **Criar o banco de dados e a tabela necess√°ria:**
  ```sql
  CREATE DATABASE crm_vendas;
  CREATE TABLE vendas (
      id SERIAL PRIMARY KEY,
      email VARCHAR(255) NOT NULL,
      data TIMESTAMP NOT NULL,
      valor NUMERIC NOT NULL,
      quantidade INTEGER NOT NULL,
      produto VARCHAR(50) NOT NULL
  );
  ```

#### **8. Criar a Conex√£o com o PostgreSQL**

- A conex√£o √© gerenciada no m√≥dulo `database.py` utilizando `psycopg2`.

### **Conclus√£o**

Este projeto de arquitetura de pipeline de dados para startups oferece uma solu√ß√£o eficiente, escal√°vel e de baixo custo para lidar com o processamento e an√°lise de grandes volumes de dados de vendas. Com a utiliza√ß√£o de ferramentas modernas como Airbyte, Kafka, Airflow, DBT e Briefer, o pipeline garante a ingest√£o, transforma√ß√£o e disponibiliza√ß√£o dos dados em camadas organizadas (Bronze, Silver, Gold), permitindo uma an√°lise colaborativa e em tempo real.

Essa arquitetura modular e flex√≠vel facilita a adapta√ß√£o e o crescimento conforme a demanda aumenta, tornando-se uma excelente escolha para startups que precisam otimizar seus processos de dados sem comprometer o or√ßamento.

